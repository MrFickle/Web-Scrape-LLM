{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Objective**\n",
        "\n",
        "Your goal is to scrape the 1975 Pacific hurricane season Wikipedia page and structure the scraped data in a DataFrame with the following columns:\n",
        "\n",
        "* hurricane_storm_name\n",
        "\n",
        "* date_start\n",
        "\n",
        "* date_end\n",
        "\n",
        "* number_of_deaths\n",
        "\n",
        "* list_of_areas_affected"
      ],
      "metadata": {
        "id": "fzrrO81NACPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Methodology** ##\n",
        "\n",
        "**Step 1**: Define the url (i.e. https://en.wikipedia.org/wiki/1975_Pacific_hurricane_season) that needs to be scraped and make use of the **requests** along with the **bs4 (BeautifulSoup)** library to fetch the HTML content of the url and then parse it. We can then use the text of the page directly.\n",
        "\n",
        "**Step 2**: Directly extract the relevant data using an LLM, using a data schema for structured output using the libraries **typing** and **pydantic**. This helps us define a specific data structure for the output of the LLM, which we can then use directly to convert our extracted data to a DataFrame with minimal code. Two classes are created, one that refers to a **Hurricane** entity with the appropriate fields: **hurricane_storm_name**, **date_start**, **date_end**, **number_of_deaths**, and **list_of_areas_affected**. The second class is the **Data** class that contains a list of **Hurricane** entities. This way, all of the data can be extracted at once in the **Data** class, instead of having to extract data for each individual hurricane separately.\n",
        "\n",
        "**Step 3**: Pick an LLM to use and setup the model and define a prompt template using the **langchain** library. In this example **\"gpt-4o-mini\"** was used because it's cheap and fast. This approach is more modular when it comes to switching between different LLMs and it is also straightforward in terms of how to structure your prompts. In this example, since we want structured data as an output, it is very easy to do with **langchain** as you can just pass the schema from the previously created **Data** class directly.\n",
        "\n",
        "**Step 4**: This is an iterable step of the process where different system prompts are defined to test using the LLM and based on the results make relevant adjustments. Typically, I start with a more generic system prompt to see what the LLM can do, and based on the performance I will consider making it more detailed, define specific instructions, provide multi-shot examples, define it as a chain-of-thought process, etc. I might also have to consider at this point whether a different LLM (better or more specific to the problem) or a different approach could be used by breaking the initial problem into smaller ones and test whether there are any improvements. In this case, I tested **\"gpt-4o\"** a little bit which did provide better results, but chose to remain with **\"gpt-4o-mini\"** due to cost. In terms of different approaches, I also tried the following:\n",
        "1. Adding an extra LLM before outputing the final results, which cleans up the original text and keeps only relevant information about hurricanes and tropical storms.\n",
        "2. Adding an extra LLM before outputing the final results, which cleans up the original text and breaks it into segments where each hurricane in the text is grouped along with its relevant information (this was done by creating an additional **Data** schema).\n",
        "3. Adding an extra LLM after outputing the final results, which does a validation check on the previous response about missing or incorrect entries.\n",
        "\n",
        "**Step 5**: Choose the best option out of the ones tried, based on cost and performance. In this case, I chose the simplest approach which is demonstrated in this notebook which can be essentially defined as **Fetch -> Parse -> LLM -> Data**. The performance of this method was as good if not better than the other ones I tried, and was also the most cost efficient as the number of tokens used are kept to a minimum and the final data are extracted with a single call. If the LLM was upgraded to **\"gpt-4o\"** the results could be even better by slightly adjusting the system prompt, but I prioritized the cost in this case."
      ],
      "metadata": {
        "id": "fpNHY_yM85xm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Install langchain** ###"
      ],
      "metadata": {
        "id": "BrHQekkEG8In"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary missing libraries\n",
        "!pip install langchain\n",
        "!pip install langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmS5dGDrC0yj",
        "outputId": "be35c16f-860d-4da2-bbbe-086ac4c34299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.34)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_core-0.3.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.121-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.0->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (4.12.2)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading langchain-0.3.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.1-py3-none-any.whl (405 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.1/405.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.121-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, h11, jsonpatch, httpcore, httpx, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.0 langchain-core-0.3.1 langchain-text-splitters-0.3.0 langsmith-0.1.121 orjson-3.10.7 tenacity-8.5.0\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.3.1)\n",
            "Collecting openai<2.0.0,>=1.40.0 (from langchain_openai)\n",
            "  Downloading openai-1.45.1-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.117 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain_openai) (0.1.121)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain_openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain_openai) (2.9.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain_openai) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (0.27.2)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain_openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.66.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.3->langchain_openai) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3->langchain_openai) (2.23.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.0.7)\n",
            "Downloading langchain_openai-0.2.0-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.45.1-py3-none-any.whl (374 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.2/374.2 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, tiktoken, openai, langchain_openai\n",
            "Successfully installed jiter-0.5.0 langchain_openai-0.2.0 openai-1.45.1 tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Setup imports, api key and define the url to scrape** ###"
      ],
      "metadata": {
        "id": "-BUaUU7_HKbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from typing import List, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "openai_api_key = ''\n",
        "\n",
        "# Define url\n",
        "url = \"https://en.wikipedia.org/wiki/1975_Pacific_hurricane_season\""
      ],
      "metadata": {
        "id": "B_H6qJN-H8vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Define scraping function** ###"
      ],
      "metadata": {
        "id": "NlwtwWKGH-TA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function fetches the url data and parses it so that they can be used by the LLM later on\n",
        "def get_url_soup(url):\n",
        "    # Send a GET request to fetch the webpage content\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
        "\n",
        "    return soup"
      ],
      "metadata": {
        "id": "IinrhrtfHV-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Define data schema** ###"
      ],
      "metadata": {
        "id": "5tlxX707IibT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Hurricane(BaseModel):\n",
        "    \"\"\"Information about a hurricane.\"\"\"\n",
        "\n",
        "    # Description of the schema Hurricane\n",
        "    # This doc-string is sent to the LLM as the description of the schema Hurricane,\n",
        "    # and it can help to improve extraction results.\n",
        "\n",
        "    hurricane_storm_name: str = Field(\n",
        "        default=None, description=\"The name of the hurricane or storm.\"\n",
        "    )\n",
        "    date_start: Optional[str] = Field(\n",
        "        default=None, description=\"The start date of the hurricane.\"\n",
        "    )\n",
        "    date_end: Optional[str] = Field(\n",
        "        default=None, description=\"The end date of the hurricane.\"\n",
        "    )\n",
        "    number_of_deaths: Optional[int] = Field(\n",
        "        default=None, description=\"The number of deaths caused by the hurricane.\"\n",
        "    )\n",
        "    list_of_areas_affected: Optional[List[str]] = Field(\n",
        "        default=None, description=\"A list of areas affected by the hurricane. Any location the hurricane passed from.\"\n",
        "    )\n",
        "\n",
        "\n",
        "class Data(BaseModel):\n",
        "    \"\"\"Extracted data about hurricanes.\"\"\"\n",
        "\n",
        "    # Creates a model so that we can extract multiple hurricane entities.\n",
        "    hurricanes: List[Hurricane]\n",
        "\n",
        "# This function converts the LLM's response to a dataframe for easier view\n",
        "def convert_response_to_df(response):\n",
        "    df = pd.DataFrame([row for row in response.model_dump()['hurricanes']])\n",
        "    return df"
      ],
      "metadata": {
        "id": "4NKC74t5n_B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Setup LLM, define system prompt, prompt template, and get output** ###"
      ],
      "metadata": {
        "id": "sYto5Dz4Iwgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    api_key=openai_api_key\n",
        ")\n",
        "\n",
        "# Define system prompt\n",
        "system_prompt = \"\"\"\n",
        "You are an AI assistant specialized in extracting structured information about hurricane storms from text. Your task is to analyze the given text and extract relevant details for all hurricane storms mentioned.\n",
        "\n",
        "General Instructions:\n",
        "1. Do not stop your response for any reason until you have extracted the relevant data for all hurricanes or tropical storms mentioned in the text. If you cannot find data for certain hurricane storms, just leave\n",
        "the relevant fields blank. However, if it makes sense to include certain information then do it, it's better to add than to omit information.\n",
        "2. If a piece of information is not available in the text and you cannot provide an answer, set the corresponding field as None. Do not attempt to fill it, unless you can infer its value indirectly.\n",
        "3. When searching for the affected areas, know that a hurricane storm forms at an initial location and then passes through different locations until it dissipates at some point, so make sure to include any relevant\n",
        "geographical information about the path the hurricane storm followed from beginning to end.  This could be information relating to specific landmarks, relative locations, areas, oceans, cities, countries, etc expressed\n",
        "either directly (e.g. \"X\") or indirectly (e.g., \"northwest of X\"), or with approximate distances (e.g., \"100mi away from X\"). The affected areas could be more than 1. List any of them that you find for each hurricane storm.\n",
        "Typically there should be at least 1 area mentioned.\n",
        "4. Ask yourself the questions:\n",
        "- Is there any location mentioned in the text where the hurricane storm started or formed at?\n",
        "- Are there any locations mentioned in the text where the hurricane storm passed from after forming?\n",
        "- Did the hurricane storm affect any locations indirectly?\n",
        "- Is the location where the hurricane storm dissipated mentioned?\n",
        "5. Make sure to include the location information as it is written in the original text. Do not omit any information.\n",
        "\n",
        "Example 1:\n",
        "\"hurricane_storm_name\": \"Tropical Storm Lee\",\n",
        "\"date_start\": \"2011-09-02\",\n",
        "\"date_end\": \"2011-09-05\",\n",
        "\"number_of_deaths\": 18,\n",
        "\"list_of_areas_affected\": [\"X\", \"100mi inland from X\", \"Northeast X\", \"250mi south from X\"]\n",
        "\n",
        "Example 2:\n",
        "\"hurricane_storm_name\": \"Hurricane Tim\",\n",
        "\"date_start\": \"2012-05-11\",\n",
        "\"date_end\": \"2012-05-14\",\n",
        "\"number_of_deaths\": 2,\n",
        "\"list_of_areas_affected\": [\"close to X\", \"east north-east of X\", \"at X\"]\n",
        "\n",
        "Example 3:\n",
        "\"hurricane_storm_name\": \"Hurricane Luna\",\n",
        "\"date_start\": \"2002-03-13\",\n",
        "\"date_end\": \"2002-03-17\",\n",
        "\"number_of_deaths\": 0,\n",
        "\"list_of_areas_affected\": [\"somewhere around X\"]\n",
        "\"\"\"\n",
        "\n",
        "# Define prompt template\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            system_prompt\n",
        "        ),\n",
        "        (\"human\", \"{text}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Scrape URL and get the text\n",
        "text = get_url_soup(url).get_text()\n",
        "\n",
        "# Create langchain runnable\n",
        "runnable = prompt | llm.with_structured_output(schema=Data)\n",
        "\n",
        "# Get response\n",
        "response = runnable.invoke({\"text\": text})\n",
        "\n",
        "# Convert response to DataFrame and print\n",
        "df = convert_response_to_df(response)\n",
        "print(df.to_markdown())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdU35PANHrqB",
        "outputId": "64cd7999-be86-40d4-98a8-d5a2a6c12073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|    | hurricane_storm_name     | date_start   | date_end   |   number_of_deaths | list_of_areas_affected                                                                                                                                        |\n",
            "|---:|:-------------------------|:-------------|:-----------|-------------------:|:--------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "|  0 | Hurricane Agatha         | 1975-06-02   | 1975-06-05 |                  0 | ['about 290 mi southwest of Acapulco', 'about 170 mi southwest of Zihuatanejo', 'about 140 mi south of the Tres Marias Islands']                              |\n",
            "|  1 | Tropical Storm Bridget   | 1975-06-28   | 1975-07-03 |                  0 | ['about 575 mi south of the tip of the Baja California Peninsula']                                                                                            |\n",
            "|  2 | Hurricane Carlotta       | 1975-07-02   | 1975-07-11 |                  0 | ['about 480 mi south of Acapulco']                                                                                                                            |\n",
            "|  3 | Hurricane Denise         | 1975-07-05   | 1975-07-15 |                  0 | ['parts of Mexico']                                                                                                                                           |\n",
            "|  4 | Tropical Storm Eleanor   | 1975-07-10   | 1975-07-12 |                  0 | ['near Manzanillo']                                                                                                                                           |\n",
            "|  5 | Tropical Storm Francene  | 1975-07-27   | 1975-07-30 |                  0 | []                                                                                                                                                            |\n",
            "|  6 | Tropical Storm Georgette | 1975-08-11   | 1975-08-14 |                  0 | []                                                                                                                                                            |\n",
            "|  7 | Tropical Storm Hilary    | 1975-08-13   | 1975-08-17 |                  0 | []                                                                                                                                                            |\n",
            "|  8 | Hurricane Ilsa           | 1975-08-18   | 1975-08-26 |                  0 | []                                                                                                                                                            |\n",
            "|  9 | Hurricane Jewel          | 1975-08-24   | 1975-08-31 |                  0 | ['about 250 mi south of Acapulco']                                                                                                                            |\n",
            "| 10 | Hurricane Katrina        | 1975-08-29   | 1975-09-07 |                  0 | ['Socorro Island']                                                                                                                                            |\n",
            "| 11 | Unnamed hurricane        | 1975-08-31   | 1975-09-05 |                  0 | ['about 930 miles northeast of Hawaii', 'about 685 miles north of Kauai', 'about 1,170 miles south of Alaska', 'about 315 miles southwest of Juneau, Alaska'] |\n",
            "| 12 | Hurricane Lily           | 1975-09-16   | 1975-09-21 |                  0 | ['about 160 mi south of Manzanillo', 'southwest of Socorro Island']                                                                                           |\n",
            "| 13 | Tropical Storm Monica    | 1975-09-28   | 1975-10-02 |                  0 | []                                                                                                                                                            |\n",
            "| 14 | Tropical Storm Nanette   | 1975-09-28   | 1975-10-04 |                  0 | []                                                                                                                                                            |\n",
            "| 15 | Hurricane Olivia         | 1975-10-22   | 1975-10-25 |                 30 | ['just south of Mazatlán']                                                                                                                                    |\n",
            "| 16 | Tropical Storm Priscilla | 1975-11-02   | 1975-11-07 |                  0 | ['about 115 mi short of landfall at Clarion Island']                                                                                                          |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. All together** ###"
      ],
      "metadata": {
        "id": "qiww4EfXJEFV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "judVjKVTABAX",
        "outputId": "9a8abb99-abbb-4975-cd41-1d9f7c60d2b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|    | hurricane_storm_name     | date_start   | date_end   |   number_of_deaths | list_of_areas_affected                                                                                                                                        |\n",
            "|---:|:-------------------------|:-------------|:-----------|-------------------:|:--------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "|  0 | Hurricane Agatha         | 1975-06-02   | 1975-06-05 |                  0 | ['about 290 mi southwest of Acapulco', 'about 170 mi southwest of Zihuatanejo', 'about 140 mi south of the Tres Marias Islands']                              |\n",
            "|  1 | Tropical Storm Bridget   | 1975-06-28   | 1975-07-03 |                  0 | ['about 575 mi south of the tip of the Baja California Peninsula']                                                                                            |\n",
            "|  2 | Hurricane Carlotta       | 1975-07-02   | 1975-07-11 |                  0 | ['about 480 mi south of Acapulco']                                                                                                                            |\n",
            "|  3 | Hurricane Denise         | 1975-07-05   | 1975-07-15 |                  0 | ['parts of Mexico']                                                                                                                                           |\n",
            "|  4 | Tropical Storm Eleanor   | 1975-07-10   | 1975-07-12 |                  0 | ['near Manzanillo']                                                                                                                                           |\n",
            "|  5 | Tropical Storm Francene  | 1975-07-27   | 1975-07-30 |                  0 | []                                                                                                                                                            |\n",
            "|  6 | Tropical Storm Georgette | 1975-08-11   | 1975-08-14 |                  0 | []                                                                                                                                                            |\n",
            "|  7 | Tropical Storm Hilary    | 1975-08-13   | 1975-08-17 |                  0 | []                                                                                                                                                            |\n",
            "|  8 | Hurricane Ilsa           | 1975-08-18   | 1975-08-26 |                  0 | []                                                                                                                                                            |\n",
            "|  9 | Hurricane Jewel          | 1975-08-24   | 1975-08-31 |                  0 | ['about 250 mi south of Acapulco']                                                                                                                            |\n",
            "| 10 | Hurricane Katrina        | 1975-08-29   | 1975-09-07 |                  0 | ['Socorro Island']                                                                                                                                            |\n",
            "| 11 | Unnamed hurricane        | 1975-08-31   | 1975-09-05 |                  0 | ['about 930 miles northeast of Hawaii', 'about 685 miles north of Kauai', 'about 1,170 miles south of Alaska', 'about 315 miles southwest of Juneau, Alaska'] |\n",
            "| 12 | Hurricane Lily           | 1975-09-16   | 1975-09-21 |                  0 | ['southwest of Socorro Island']                                                                                                                               |\n",
            "| 13 | Tropical Storm Monica    | 1975-09-28   | 1975-10-02 |                  0 | []                                                                                                                                                            |\n",
            "| 14 | Tropical Storm Nanette   | 1975-09-28   | 1975-10-04 |                  0 | []                                                                                                                                                            |\n",
            "| 15 | Hurricane Olivia         | 1975-10-22   | 1975-10-25 |                 30 | ['just south of Mazatlán']                                                                                                                                    |\n",
            "| 16 | Tropical Storm Priscilla | 1975-11-02   | 1975-11-07 |                  0 | ['about 115 mi short of landfall near Clarion Island']                                                                                                        |\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from typing import List, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "openai_api_key = ''\n",
        "\n",
        "# Define url\n",
        "url = \"https://en.wikipedia.org/wiki/1975_Pacific_hurricane_season\"\n",
        "\n",
        "# This function fetches the url data and parses it so that they can be used by the LLM later on\n",
        "def get_url_soup(url):\n",
        "    # Send a GET request to fetch the webpage content\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
        "\n",
        "    return soup\n",
        "\n",
        "# This function converts the LLM's response to a dataframe for easier view\n",
        "def convert_response_to_df(response):\n",
        "    df = pd.DataFrame([row for row in response.model_dump()['hurricanes']])\n",
        "    return df\n",
        "\n",
        "class Hurricane(BaseModel):\n",
        "    \"\"\"Information about a hurricane.\"\"\"\n",
        "\n",
        "    # Description of the schema Hurricane\n",
        "    # This doc-string is sent to the LLM as the description of the schema Hurricane,\n",
        "    # and it can help to improve extraction results.\n",
        "\n",
        "    hurricane_storm_name: str = Field(\n",
        "        default=None, description=\"The name of the hurricane or storm.\"\n",
        "    )\n",
        "    date_start: Optional[str] = Field(\n",
        "        default=None, description=\"The start date of the hurricane.\"\n",
        "    )\n",
        "    date_end: Optional[str] = Field(\n",
        "        default=None, description=\"The end date of the hurricane.\"\n",
        "    )\n",
        "    number_of_deaths: Optional[int] = Field(\n",
        "        default=None, description=\"The number of deaths caused by the hurricane.\"\n",
        "    )\n",
        "    list_of_areas_affected: Optional[List[str]] = Field(\n",
        "        default=None, description=\"A list of areas affected by the hurricane. Any location the hurricane passed from.\"\n",
        "    )\n",
        "\n",
        "\n",
        "class Data(BaseModel):\n",
        "    \"\"\"Extracted data about hurricanes.\"\"\"\n",
        "\n",
        "    # Creates a model so that we can extract multiple hurricane entities.\n",
        "    hurricanes: List[Hurricane]\n",
        "\n",
        "# Setup LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    api_key=openai_api_key\n",
        ")\n",
        "\n",
        "# Define system prompt\n",
        "system_prompt = \"\"\"\n",
        "You are an AI assistant specialized in extracting structured information about hurricane storms from text. Your task is to analyze the given text and extract relevant details for all hurricane storms mentioned.\n",
        "\n",
        "General Instructions:\n",
        "1. Do not stop your response for any reason until you have extracted the relevant data for all hurricanes or tropical storms mentioned in the text. If you cannot find data for certain hurricane storms, just leave\n",
        "the relevant fields blank. However, if it makes sense to include certain information then do it, it's better to add than to omit information.\n",
        "2. If a piece of information is not available in the text and you cannot provide an answer, set the corresponding field as None. Do not attempt to fill it, unless you can infer its value indirectly.\n",
        "3. When searching for the affected areas, know that a hurricane storm forms at an initial location and then passes through different locations until it dissipates at some point, so make sure to include any relevant\n",
        "geographical information about the path the hurricane storm followed from beginning to end.  This could be information relating to specific landmarks, relative locations, areas, oceans, cities, countries, etc expressed\n",
        "either directly (e.g. \"X\") or indirectly (e.g., \"northwest of X\"), or with approximate distances (e.g., \"100mi away from X\"). The affected areas could be more than 1. List any of them that you find for each hurricane storm.\n",
        "Typically there should be at least 1 area mentioned.\n",
        "4. Ask yourself the questions:\n",
        "- Is there any location mentioned in the text where the hurricane storm started or formed at?\n",
        "- Are there any locations mentioned in the text where the hurricane storm passed from after forming?\n",
        "- Did the hurricane storm affect any locations indirectly?\n",
        "- Is the location where the hurricane storm dissipated mentioned?\n",
        "5. Make sure to include the location information as it is written in the original text. Do not omit any information.\n",
        "\n",
        "Example 1:\n",
        "\"hurricane_storm_name\": \"Tropical Storm Lee\",\n",
        "\"date_start\": \"2011-09-02\",\n",
        "\"date_end\": \"2011-09-05\",\n",
        "\"number_of_deaths\": 18,\n",
        "\"list_of_areas_affected\": [\"X\", \"100mi inland from X\", \"Northeast X\", \"250mi south from X\"]\n",
        "\n",
        "Example 2:\n",
        "\"hurricane_storm_name\": \"Hurricane Tim\",\n",
        "\"date_start\": \"2012-05-11\",\n",
        "\"date_end\": \"2012-05-14\",\n",
        "\"number_of_deaths\": 2,\n",
        "\"list_of_areas_affected\": [\"close to X\", \"east north-east of X\", \"at X\"]\n",
        "\n",
        "Example 3:\n",
        "\"hurricane_storm_name\": \"Hurricane Luna\",\n",
        "\"date_start\": \"2002-03-13\",\n",
        "\"date_end\": \"2002-03-17\",\n",
        "\"number_of_deaths\": 0,\n",
        "\"list_of_areas_affected\": [\"somewhere around X\"]\n",
        "\"\"\"\n",
        "\n",
        "# Define prompt template\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            system_prompt\n",
        "        ),\n",
        "        (\"human\", \"{text}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Scrape URL and get the text\n",
        "text = get_url_soup(url).get_text()\n",
        "\n",
        "# Create langchain runnable\n",
        "runnable = prompt | llm.with_structured_output(schema=Data)\n",
        "\n",
        "# Get response\n",
        "response = runnable.invoke({\"text\": text})\n",
        "\n",
        "# Convert response to DataFrame and print\n",
        "df = convert_response_to_df(response)\n",
        "print(df.to_markdown())"
      ]
    }
  ]
}