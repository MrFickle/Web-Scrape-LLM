# Web Scraping with Large Language Models (LLMs) - Python Notebook

This Python Notebook demonstrates a web scraping workflow using large language models (LLMs). It includes scraping web data from the 1975 Pacific hurricane season Wikipedia page, processing it, and utilizing **GPT-4o-mini** to extract relevant information and print it in a structured format.

## Key Features
- Web scraping the [1975 Pacific hurricane season Wikipedia page](https://en.wikipedia.org/wiki/1975_Pacific_hurricane_season).
- Use an LLM to extract relevant information with structured output.

## Running the Notebook
This notebook is designed to run in **Google Colab** for convenience, with all necessary libraries pre-installed in the environment. Simply upload the notebook to Colab and execute the cells sequentially.

### Running Locally
If you prefer to run this notebook locally, you'll need to install the required dependencies. Make sure you have Python installed, then use the `requirements.txt` file to install the dependencies.

### Steps to Run Locally:
1. Clone the repository or download the notebook and the `requirements.txt` file.
2. Install the required dependencies by running:

   ```bash
   pip install -r requirements.txt
